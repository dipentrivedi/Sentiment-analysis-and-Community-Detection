{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createData(file):\n",
    "    fileReader = open(file,\"r\")\n",
    "\n",
    "    cnt=0\n",
    "    for r in fileReader:\n",
    "        tokens= tokenize(r)\n",
    "        score = 0\n",
    "        for t in tokens:\n",
    "            try:\n",
    "                synset = list(swn.senti_synsets(t))[0]\n",
    "                score = score + synset.pos_score() - synset.neg_score()\n",
    "            \n",
    "            except(IndexError):\n",
    "                pass\n",
    "\n",
    "        if(score > 0):\n",
    "            fPos=open(\"data/pos/\"+str(cnt)+\"_1.txt\",\"w\")\n",
    "            cnt+=1\n",
    "            fPos.write(r)\n",
    "            fPos.close()\n",
    "        elif(score < 0):\n",
    "            fNeg=open(\"data/neg/\"+str(cnt)+\"_0.txt\",\"w\")\n",
    "            cnt+=1\n",
    "            fNeg.write(r)\n",
    "            fNeg.close()\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \"\"\"\n",
    "    Walks all subdirectories of this path and reads all\n",
    "    the text files and labels.\n",
    "    DONE ALREADY.\n",
    "\n",
    "    Params:\n",
    "      path....path to files\n",
    "    Returns:\n",
    "      docs.....list of strings, one per document\n",
    "      labels...list of ints, 1=positive, 0=negative label.\n",
    "               Inferred from file path (i.e., if it contains\n",
    "               'pos', it is 1, else 0)\n",
    "    \"\"\"\n",
    "    data=[]\n",
    "    \n",
    "    resuldDict={}\n",
    "    \n",
    "    #get Posisitve File records\n",
    "    posFnames = sorted([f for f in glob.glob(os.path.join(path, 'pos', '*.txt'))])\n",
    "    resuldDict['posFiles']=len(posFnames)\n",
    "    resuldDict['posRecord']=open(posFnames[0]).readlines()[0].strip()\n",
    "    for f1 in sorted(posFnames):\n",
    "        with open(f1,'r') as fp:\n",
    "            data.append((1,fp.readlines()[0].strip(\"\\n\"))) \n",
    "            \n",
    "            \n",
    "#     data = [(1, open(f).readlines()[0].strip(\"\\n\")) for f in sorted(posFnames)]\n",
    "    \n",
    "    #get Negative File records\n",
    "    negFnames = [f for f in glob.glob(os.path.join(path, 'neg', '*.txt'))]\n",
    "    resuldDict['negFiles']=len(negFnames)\n",
    "    resuldDict['negRecord']=open(negFnames[2]).readlines()[0].strip()\n",
    "    for f2 in sorted(negFnames):\n",
    "        with open(f2,'r') as fp:\n",
    "            data.append((0,fp.readlines()[0].strip(\"\\n\"))) \n",
    "#     data += [(0, open(f).readlines()[0].strip(\"\\n\")) for f in sorted(negFnames)]\n",
    "    \n",
    "    data = sorted(data, key=lambda x: x[1])\n",
    "\n",
    "    #Write Classifier answers to the file\n",
    "    opFile=open(path+\"/classifierAnswers.txt\",\"w+\")\n",
    "    flag=True\n",
    "    for r in resuldDict:\n",
    "        if flag:\n",
    "            opFile.write(r+\":\"+str(resuldDict[r]))\n",
    "            flag=False\n",
    "        else:\n",
    "            opFile.write(\"\\n\"+r+\":\"+str(resuldDict[r]))\n",
    "\n",
    "    opFile.close()\n",
    "\n",
    "    return np.array([d[1] for d in data]), np.array([d[0] for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(doc, keep_internal_punct=False):\n",
    "    \"\"\"\n",
    "    Tokenize a string.\n",
    "    The string should be converted to lowercase.\n",
    "    If keep_internal_punct is False, then return only the alphanumerics (letters, numbers and underscore).\n",
    "    If keep_internal_punct is True, then also retain punctuation that\n",
    "    is inside of a word. E.g., in the example below, the token \"isn't\"\n",
    "    is maintained when keep_internal_punct=True; otherwise, it is\n",
    "    split into \"isn\" and \"t\" tokens.\n",
    "\n",
    "    Params:\n",
    "      doc....a string.\n",
    "      keep_internal_punct...see above\n",
    "    Returns:\n",
    "      a numpy array containing the resulting tokens.\n",
    "\n",
    "    >>> tokenize(\" Hi there! Isn't this fun?\", keep_internal_punct=False)\n",
    "    array(['hi', 'there', 'isn', 't', 'this', 'fun'], \n",
    "          dtype='<U5')\n",
    "    >>> tokenize(\"Hi there! Isn't this fun? \", keep_internal_punct=True)\n",
    "    array(['hi', 'there', \"isn't\", 'this', 'fun'], \n",
    "          dtype='<U5')\n",
    "     >>> tokenize(\"??necronomicon?? geträumte sünden.<br>Hi\", True)\n",
    "     array(['necronomicon', 'geträumte', 'sünden.<br>hi'], \n",
    "           dtype='<U13')\n",
    "     >>> tokenize(\"??necronomicon?? geträumte sünden.<br>Hi\", False)\n",
    "     array(['necronomicon', 'geträumte', 'sünden', 'br', 'hi'], \n",
    "           dtype='<U12')      \n",
    "\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    \n",
    "    t=[]\n",
    "    if(keep_internal_punct):\n",
    "        for i in doc.split():\n",
    "            if(i):\n",
    "                t.append(re.sub(r\"^\\W+|\\W+$\", \"\", i))\n",
    "    else:\n",
    "\n",
    "        t.extend(re.findall('[^\\W\\s]+', doc))\n",
    "    return(np.array([x.lower() for x in t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_vectorize(docs, tokenizer_fn=tokenize, min_df=1,\n",
    "                 max_df=1., binary=True, ngram_range=(1,1)):\n",
    "    \"\"\"\n",
    "    Convert a list of filenames into a sparse csr_matrix, where\n",
    "    each row is a file and each column represents a unique word.\n",
    "    Use sklearn's CountVectorizer: http://goo.gl/eJ2PJ5\n",
    "    Params:\n",
    "        filenames.......list of review file names\n",
    "        tokenizer_fn....the function used to tokenize each document\n",
    "        min_df..........remove terms from the vocabulary that don't appear\n",
    "                        in at least this many documents\n",
    "        max_df..........remove terms from the vocabulary that appear in more\n",
    "                        than this fraction of documents\n",
    "        binary..........If true, each documents is represented by a binary\n",
    "                        vector, where 1 means a term occurs at least once in \n",
    "                        the document. If false, the term frequency is used instead.\n",
    "        ngram_range.....A tuple (n,m) means to use phrases of length n to m inclusive.\n",
    "                        E.g., (1,2) means consider unigrams and bigrams.\n",
    "    Return:\n",
    "        A tuple (X, vec), where X is the csr_matrix of feature vectors,\n",
    "        and vec is the CountVectorizer object.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    vec = CountVectorizer(input='docs', tokenizer=tokenizer_fn,\n",
    "                          binary=binary, min_df=min_df, max_df=max_df,\n",
    "                          ngram_range=ngram_range)\n",
    "\n",
    "    X = vec.fit_transform(docs)\n",
    "    return (X, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_score(truth, predicted):\n",
    "    \"\"\" Compute accuracy of predictions.\n",
    "    DONE ALREADY\n",
    "    Params:\n",
    "      truth.......array of true labels (0 or 1)\n",
    "      predicted...array of predicted labels (0 or 1)\n",
    "    \"\"\"\n",
    "    return len(np.where(truth==predicted)[0]) / len(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_validation_accuracy(clf, X, labels, k):\n",
    "    \"\"\"\n",
    "    Compute the average testing accuracy over k folds of cross-validation. You\n",
    "    can use sklearn's KFold class here (no random seed, and no shuffling\n",
    "    needed).\n",
    "\n",
    "    Params:\n",
    "      clf......A LogisticRegression classifier.\n",
    "      X........A csr_matrix of features.\n",
    "      labels...The true labels for each instance in Xh\n",
    "      k........The number of cross-validation folds.\n",
    "\n",
    "    Returns:\n",
    "      The average testing accuracy of the classifier\n",
    "      over each fold of cross-validation.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "   \n",
    "    kf = KFold(len(labels),n_folds=k,shuffle=True)\n",
    "    ac = []    \n",
    "    train_ac = []\n",
    "    for fIndex , (train, test) in enumerate(kf):\n",
    "        \n",
    "        clf.fit(X[train], labels[train])\n",
    "        train_ac.append(accuracy_score(labels[train], clf.predict(X[train])))\n",
    "        predicted=clf.predict(X[test])\n",
    "        ac1=accuracy_score(labels[test], predicted)\n",
    "        ac.append(ac1)\n",
    "\n",
    "    return np.mean(ac), np.mean(train_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Put it all together.\n",
    "    ALREADY DONE.\n",
    "    \"\"\"\n",
    "    startTime = time.clock()\n",
    "    \n",
    "    if not os.path.exists('data/pos'):\n",
    "        os.makedirs('data/pos')\n",
    "    if not os.path.exists('data/neg'):\n",
    "        os.makedirs('data/neg')\n",
    "    \n",
    "    # Do sentiment analysis on movie reviews\n",
    "    createData(\"data/movie_reviews.txt\")\n",
    "    \n",
    "    # read data.\n",
    "    docs, labels = read_data(\"data\")\n",
    "\n",
    "    # Do vectorize\n",
    "    matrix, vec = do_vectorize(docs)\n",
    "\n",
    "    # Do k-fold Cross validation\n",
    "    print('average cross validation test accuracy=%.4f train accuracy=%.4f' %\n",
    "      cross_validation_accuracy(LogisticRegression(random_state=42, C=1, penalty='l2'),matrix,labels,5))\n",
    "    endTime =  time.clock()\n",
    "    \n",
    "    print(\"Total running time :%f\"%(endTime -  startTime))\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
